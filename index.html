<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Reading Fluency (Whisper)</title>
  <style>
    textarea { width: 100%; height: 100px; }
    .word.correct { color: green; font-weight: bold; }
    .word.incorrect { color: red; font-weight: bold; }
  </style>
</head>
<body>
  <h1>Reading Fluency Analyzer with Whisper</h1>

  <textarea id="textInput">The quick brown fox jumps over the lazy dog.</textarea>
  <br />
  <button id="startBtn">Start Recording</button>
  <button id="stopBtn" disabled>Stop Recording</button>
  <p>Duration: <span id="duration">0</span> seconds</p>
  <p>WPM: <span id="wpm">0</span></p>
  <p>WCPM: <span id="wcpm">0</span></p>

  <h3>Speech Recognition Result:</h3>
  <div id="outputText">[Result will appear here]</div>

  <script>
    let mediaRecorder;
    let audioChunks = [];
    let startTime, endTime;

    const startBtn = document.getElementById("startBtn");
    const stopBtn = document.getElementById("stopBtn");
    const durationDisplay = document.getElementById("duration");
    const wpmDisplay = document.getElementById("wpm");
    const wcpmDisplay = document.getElementById("wcpm");
    const outputText = document.getElementById("outputText");

    startBtn.onclick = async () => {
      audioChunks = [];
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.start();
      startTime = new Date();

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      startBtn.disabled = true;
      stopBtn.disabled = false;
    };

    stopBtn.onclick = async () => {
      endTime = new Date();
      mediaRecorder.stop();

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
        const file = new File([audioBlob], "speech.webm", { type: "audio/webm" });

        const formData = new FormData();
        formData.append("file", file);
        formData.append("model", "whisper-1");
        formData.append("response_format", "text");
        formData.append("language", "en");

        const openaiApiKey = "YOUR_OPENAI_API_KEY"; // ← 여기에 본인의 OpenAI API 키 입력

        const response = await fetch("https://api.openai.com/v1/audio/transcriptions", {
          method: "POST",
          headers: {
            Authorization: `Bearer ${openaiApiKey}`
          },
          body: formData
        });

        const transcript = await response.text();
        analyze(transcript);
      };

      startBtn.disabled = false;
      stopBtn.disabled = true;
    };

    function analyze(transcript) {
      const original = document.getElementById("textInput").value.trim().split(/\\s+/);
      const spoken = transcript.trim().split(/\\s+/);
      let correct = 0;

      const result = original.map((word, i) => {
        const spokenWord = spoken[i] || "";
        const match = word.toLowerCase() === spokenWord.toLowerCase();
        if (match) correct++;
        return `<span class=\"word ${match ? "correct" : "incorrect"}\">${word}</span>`;
      });

      outputText.innerHTML = result.join(" ");

      const duration = (endTime - startTime) / 1000;
      durationDisplay.textContent = duration.toFixed(1);
      wpmDisplay.textContent = ((spoken.length / duration) * 60).toFixed(1);
      wcpmDisplay.textContent = ((correct / duration) * 60).toFixed(1);
    }
  </script>
</body>
</html>
